#!/usr/bin/env bash
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

if [ -z "${SPARK_HOME}" ]; then
  source "$(dirname "$0")"/find-spark-home
fi

. "${SPARK_HOME}"/bin/load-spark-env.sh

# Find the java binary
if [ -n "${JAVA_HOME}" ]; then
  RUNNER="${JAVA_HOME}/bin/java"
else
  if [ "$(command -v java)" ]; then
    RUNNER="java"
  else
    echo "JAVA_HOME is not set" >&2
    exit 1
  fi
fi

# Find Spark jars.
if [ -d "${SPARK_HOME}/jars" ]; then
  SPARK_JARS_DIR="${SPARK_HOME}/jars"
else
  SPARK_JARS_DIR="${SPARK_HOME}/assembly/target/scala-$SPARK_SCALA_VERSION/jars"
fi

if [ ! -d "$SPARK_JARS_DIR" ] && [ -z "$SPARK_TESTING$SPARK_SQL_TESTING" ]; then
  echo "Failed to find Spark jars directory ($SPARK_JARS_DIR)." 1>&2
  echo "You need to build Spark with the target \"package\" before running this program." 1>&2
  exit 1
else
  LAUNCH_CLASSPATH="$SPARK_JARS_DIR/*"
fi

# Add the launcher build dir to the classpath if requested.
if [ -n "$SPARK_PREPEND_CLASSES" ]; then
  LAUNCH_CLASSPATH="${SPARK_HOME}/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH"
fi

# For tests
if [[ -n "$SPARK_TESTING" ]]; then
  unset YARN_CONF_DIR
  unset HADOOP_CONF_DIR
fi

# The launcher library will print arguments separated by a NULL character, to allow arguments with
# characters that would be otherwise interpreted by the shell. Read that in a while loop, populating
# an array that will be used to exec the final command.
#
# The exit code of the launcher is appended to the output, so the parent shell removes it from the
# command array and checks the value to see if the launcher succeeded.
build_command() {
  "$RUNNER" -Xmx128m $SPARK_LAUNCHER_OPTS -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"
  printf "%d\0" $?
}

# Turn off posix mode since it does not allow process substitution
set +o posix
CMD=()
DELIM=$'\n'
CMD_START_FLAG="false"
while IFS= read -d "$DELIM" -r ARG; do
  if [ "$CMD_START_FLAG" == "true" ]; then
    CMD+=("$ARG")
  else
    if [ "$ARG" == $'\0' ]; then
      # After NULL character is consumed, change the delimiter and consume command string.
      DELIM=''
      CMD_START_FLAG="true"
    elif [ "$ARG" != "" ]; then
      echo "$ARG"
    fi
  fi
done < <(build_command "$@")

echo "fortest1" > /tmp/executor.final_cmd 
COUNT=${#CMD[@]}
LAST=$((COUNT - 1))
LAUNCHER_EXIT_CODE=${CMD[$LAST]}

# Certain JVM failures result in errors being printed to stdout (instead of stderr), which causes
# the code that parses the output of the launcher to get confused. In those cases, check if the
# exit code is an integer, and if it's not, handle it as a special error case.
if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then
  echo "${CMD[@]}" | head -n-1 1>&2
  exit 1
fi

if [ $LAUNCHER_EXIT_CODE != 0 ]; then
  exit $LAUNCHER_EXIT_CODE
fi

CMD=("${CMD[@]:0:$LAST}")

# === Conditional Gramine Execution ===

if [[ "${CMD[*]}" == *"org.apache.spark.executor.CoarseGrainedExecutorBackend"* ]]; then
  echo "[spark-class] ðŸš€ Launching Spark executor inside enclave..."

  cd $SPARK_HOME/enclave
  GRAMINE_CMD="gramine-direct"
  MANIFEST="java.manifest"
  [ "${SGX:-0}" = "1" ] && {
    GRAMINE_CMD="gramine-sgx"
    MANIFEST="java.manifest.sgx"
  }

  export SGX="${SGX:-0}"
  export stack_size="${STACK_SIZE:-4M}"
  export brk_size="${BRK_SIZE:-128M}"
  export fds_limit="${FDS_LIMIT:-1024}"
  export sgx_max_threads="${MAX_THREADS:-8}"
  export sgx_enclave_size="${SGX_ENCLAVE_SIZE:-4}G"
  export enable_sigterm="${ENABLE_SIGTERM:-1}"
  export disallow_subprocesses="${DISALLOW_SUBPROCESSES:-1}"
  export disable_aslr="${DISABLE_ASLR:-1}"

  # === Manifest Rebuild Logic ===
  CONFIG_HASH_FILE="/enclave/.last_manifest_config"
  CURRENT_HASH=$(echo "$SGX $stack_size $brk_size $fds_limit $sgx_max_threads $sgx_enclave_size $enable_sigterm $disallow_subprocesses $disable_aslr" | sha256sum | cut -d ' ' -f 1)
  REBUILD_MANIFEST=1

  if [ -f "$CONFIG_HASH_FILE" ]; then
    LAST_HASH=$(cat "$CONFIG_HASH_FILE")
    if [ "$CURRENT_HASH" = "$LAST_HASH" ] && [ -f "/enclave/$MANIFEST" ]; then
      echo "[spark-class] ðŸ§© Manifest config unchanged, skipping rebuild."
      REBUILD_MANIFEST=0
    fi
  fi

  if [ "$REBUILD_MANIFEST" = "1" ]; then
    echo "[spark-class] ðŸ—ï¸ Rebuilding manifest..."
    make -C /enclave clean all \
      WORKER_ID="$WORKER_ID" \
      SGX="$SGX" \
      stack_size="$stack_size" \
      brk_size="$brk_size" \
      fds_limit="$fds_limit" \
      sgx_max_threads="$sgx_max_threads" \
      sgx_enclave_size="$sgx_enclave_size" \
      enable_sigterm="$enable_sigterm" \
      disallow_subprocesses="$disallow_subprocesses" \
      disable_aslr="$disable_aslr"
    echo "$CURRENT_HASH" > "$CONFIG_HASH_FILE"
  fi

  # === Debug: Save Final Command ===
  echo "[spark-class] ðŸ“ Final enclave command:" > /tmp/executor.final_cmd
  printf '%q ' "$GRAMINE_CMD" "${CMD[@]}" >> /tmp/executor.final_cmd
  echo >> /tmp/executor.final_cmd

  #exec "$GRAMINE_CMD" "${CMD[@]}"
  exec "{CMD[@]}"
else
  echo  "just for check" >> /tmp/executor.final_cmd
  exec "${CMD[@]}"
fi